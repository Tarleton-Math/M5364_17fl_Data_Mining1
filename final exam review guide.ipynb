{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining I, Fall 2017 Final Exam Review Guide\n",
    "timestamp: 12/6/17, 8:06AM\n",
    "\n",
    "- What is the difference between supervised and unsupervised algorithms?\n",
    "- What is the difference between an eager learner and a lazy learner?\n",
    "- What is the difference between an classification and regression?\n",
    "- What is a confusion matrix?  If given one, interpret the results.\n",
    "- What is the bias vs variance tradeoff?  Describe/identify overfitting and underfitting.\n",
    "\n",
    "- Classify variables as\n",
    "    - Continuous, categorical, ordinal\n",
    "    - Feature, target\n",
    "\n",
    "\n",
    "- Preprocessing\n",
    "    - Describe options for handling missing data.  Ex: Drop the observation, fill with constant value, fill with variable mean, forward fill, back fill\n",
    "    - What is one-hot encoding?  When do you use it?\n",
    "    \n",
    "\n",
    "\n",
    "- Answer these questions for each algorithm below\n",
    "    - Supervised or Unsupervised?\n",
    "    - Eager or Lazy?\n",
    "    - What job does it do for a data scientist?  Ex: classifier, find clusters, reduces dimension, etc\n",
    "    - When would you use it?  Ex: Classification tasks with continuous features\n",
    "    - Describe the major hyperparamter(s)?  Ex: For PCA, it is the number of principles components that we keep.  For knn, it is $k$ = number of neighest neighbors that get to vote on the class of the new observation.\n",
    "    - How does it work?  Roughly speaking, not in deep detail.  Ex: In $k$-means, we make an initial guess of $k$ cluster centers.  Then we run the Expectation-Maximization algorithm where:\n",
    "          1. Classify each observation by which center is closest\n",
    "          2. Move each center to the midpoint of the observations in its cluster\n",
    "          3. Repeat until centers (essentially) stop moving\n",
    "    - How does it make the prediction?  Ex: SVM - which side of the dividing hyperplane the new observation lies on\n",
    "    - Limitations and (if possible) names of algorithms that improve it.  Ex: PCA only find \"flat\" structure.  But manifold learners like Isomap and Local Linear Embedding can find more general shapes.\n",
    "\n",
    "Algorithms:\n",
    "- Principle Components Analysis\n",
    "- $k$-means\n",
    "- $k$-nearest neighbors\n",
    "- Naive Bayes\n",
    "- Decision Trees\n",
    "- Random Forests (bagging ensemble version - sklearn implements a more refined version using boosting, the exam will just ask about the simpler bagging version)\n",
    "- Support Vector Machines (have not discussed hyperparameters yet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Unsupervised Algorithms\n",
    "\n",
    "- Supervised Algorithms\n",
    "  -\n",
    "  - k-Nearest Neighbors\n",
    "  - Naive Bayes\n",
    "\n",
    "\n",
    "- Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
