{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "\n",
    "\n",
    "# common activation functions and their derivatives\n",
    "def id(x):\n",
    "    return x\n",
    "\n",
    "def id_deriv(x):\n",
    "    return float(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    y = np.zeros_like(x)\n",
    "    idx = x > -700\n",
    "    y[idx] = 1 / (1 + np.exp(-1*x[idx]))\n",
    "    return y\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "\n",
    "rnd = np.random.RandomState(1)\n",
    "\n",
    "## example from https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/, use sigmoid, loss=(e**2)/2 in loss, loss_deriv=e\n",
    "feat = np.array([.05,.1]).reshape(1,-1)\n",
    "true = np.array([.01,.99]).reshape(1,-1)\n",
    "\n",
    "n, p = feat.shape\n",
    "m, q = true.shape\n",
    "if m != n:\n",
    "    raise Exception('feat and true must have the same number of rows')\n",
    "\n",
    "\n",
    "hidden_nodes = [2]\n",
    "max_steps = 1\n",
    "learn_rate = 0.5\n",
    "\n",
    "nodes = [p] + hidden_nodes + [q]\n",
    "layers = len(nodes)\n",
    "\n",
    "## set activation functions\n",
    "activation = [sigmoid for l in range(layers)]\n",
    "activation_deriv = [sigmoid_deriv for l in range(layers)]\n",
    "activation[0] = id\n",
    "activation_deriv[0] = id_deriv\n",
    "# activation[-1] = id\n",
    "# activation_deriv[-1] = id_deriv\n",
    "\n",
    "## Usually should scale data to (0,1) during preprocessing.  Skip for now for simplicity.\n",
    "feat_sc = feat.copy()\n",
    "true_sc = true.copy()\n",
    "\n",
    "\n",
    "## pre-allocate\n",
    "## X[h,i,j] = input to node j of layer h for observation i; X[h] has shape n x nodes[h]\n",
    "X = [np.zeros(shape=[n,p]) for p in nodes]\n",
    "\n",
    "## Y[h,i,j] = output from node j of layer h for observation i; Y[h] has shape n x nodes[h]\n",
    "Y = X.copy()\n",
    "\n",
    "## DLDX[h,i,j] = partial derivative of loss wrt X[h,i,j]; DLDX[h] has shape n x nodes[h]\n",
    "DLDX = X.copy()\n",
    "\n",
    "## DLDY[h,i,j] = partial derivative of loss wrt Y[h,i,j]; DLDY[h] has shape n x nodes[h]\n",
    "DLDY = X.copy()\n",
    "\n",
    "## B[h,j] = bias into node j of layer h; B[h] has shape nodes[h]\n",
    "B = [rnd.rand(p) for p in nodes]\n",
    "B[0] *= 0.0\n",
    "\n",
    "## DLDB[h,i,j] = partial derivative of loss wrt B[h,j]; DLDB[h] has shape nodes[h]\n",
    "DLDB = [np.zeros_like(b) for b in B]\n",
    "\n",
    "## W_sh[h] = (nodes[h], nodes[h+1])\n",
    "W_sh = [(i,o) for (i,o) in zip(nodes[:-1],nodes[1:])]\n",
    "\n",
    "## W[h,j,k] = weight of edge from node j of layer h to node k of layer h+1; W[h] has shape nodes[h] x nodes[h+1]\n",
    "W = [rnd.rand(*ws) for ws in W_sh]\n",
    "\n",
    "## DLDW[h,j,k] = partial derivative of loss wrt W[h,j,k]; DLDW[h] has shape nodes[h] x nodes[h+1]\n",
    "DLDW = [np.zeros_like(w) for w in W]\n",
    "\n",
    "\n",
    "## example from https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/, use sigmoid, loss=(e**2)/2 in loss, loss_deriv=e\n",
    "W = [np.array([[0.15, 0.25], [0.20, 0.30]]), np.array([[0.40, 0.50], [0.45, 0.55]])]\n",
    "B = [np.array([0.0, 0.0]), np.array([0.35, 0.35]), np.array([0.60, 0.60])]\n",
    "\n",
    "\n",
    "X[0] = feat_sc\n",
    "for step in range(max_steps):\n",
    "    forward_propagate()\n",
    "    print(Y)\n",
    "    e, L = error()    \n",
    "    if L.max() < 1e-4:\n",
    "        break\n",
    "    backward_propagate()\n",
    "    descend_gradient()\n",
    "pred_sc = Y[-1].copy()\n",
    "for (w,x,y) in zip(W,X,Y):\n",
    "    print()\n",
    "    print(x)\n",
    "    print()\n",
    "    print(w)\n",
    "    print()\n",
    "    print(y)\n",
    "    print()\n",
    "print(Y[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
