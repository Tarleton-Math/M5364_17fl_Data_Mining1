{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math 5366 - Data Mining 2, Tarleton State University, Spring 2018, Dr. Scott Cook\n",
    "\n",
    "Due Feb 1, 2018\n",
    "\n",
    "1) Load the Wisconsin breast cancer dataset \n",
    "  - from sklearn.datasets import load_breast_cancer\n",
    "  - data = load_breast_cancer()\n",
    "  \n",
    "This dataset contains 30 pieces of data for ~600 breast masses and whether it was benign or cancerous.  We'll train several supprt vector machine classifiers for this dataset.  Recall the major hyperparameters are\n",
    "  - kernel - we'll consider ['linear', 'rbf', 'sigmoid']\n",
    "  - C - we'll consider [1, 10, 50, 100, 200]\n",
    "\n",
    "Refering to Python Data Science Handbook (PDSH) section 5.3, use GridSearchCV to fit SVC using each of the 12 (kernel, C) pairs.  Recall it is *essential* to use cross-validation for *each* of these 12 runs to get a reliable accuracy estimate (cross-validation was absent from some final projects last semester.  Please do not make the mistake again).  Luckily, GridSearchCV does this for us automatically via the cv parameter.  Please use cv=5.  Please report ONLY the best (kernel,C) pair and its best accuracy.\n",
    "\n",
    "For the rest of this hwk set, we will apply different pre-processing and then repeating this gridsearch.  You may want to write a reusable function to save effort.  If you do so, you could optionally stick %time before the call to get timing benchmarks as well (optional).\n",
    "\n",
    "2) Now, read http://scikit-learn.org/stable/modules/svm.html#svm-classification.\n",
    "\n",
    "Specifically note section 1.4.5, bullet 4.  It says that for SVM, we are strongly advised to scale each feature first.  If one feature ranges [0,3] and another [0,10000], the second could swamp the first.  While the range is not so extreme here, we should get into this habit.  Please use the MinMaxScaler to linearly scale each column into the range [0,1].\n",
    "\n",
    "http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
    "\n",
    "Repeat problem 1 with the scaled data.  Please report ONLY the best (kernel,C) pair and its best accuracy.  Did this improve things?  (Suggestion, try other scaling methods too.  Do any of them work markedly better?)\n",
    "\n",
    "3) Now, using the original (non-scaled) data, apply PCA (ref PDSH section 5.9).  First, use\n",
    "  - pca = PCA(n_components=30)\n",
    "  - PC = pca.fit(data.data)\n",
    "  - print(PC.explained_variance\\_ratio_.cumsum())\n",
    "\n",
    "Note how the first component alone accounts for 98.2% of the variance!!  Let's use n_components=2, which explains 99.8%.  With this dramatically reduced dataset (30 features -> 2 features), please repeat problem 1 yet again, reporting the best (kernel, C) pair and its accuracy.  This time, please ALSO make the graphic showing the decision boundary using the code (plot_svc_decision_function) in PDHS in 5.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your responses go here:\n",
    "1. best (kernel, C) pair = (   ,   ) with accuracy = \n",
    "2. best (kernel, C) pair = (   ,   ) with accuracy = \n",
    "3. best (kernel, C) pair = (   ,   ) with accuracy = \n",
    "\n",
    "Copy of decision boundary image here.  (One option is to export graphic after you've created it below and embed copy here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
