{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. d\n",
    "5. g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math 5366 - Data Mining 2, Tarleton State University, Spring 2018, Dr. Scott Cook\n",
    "\n",
    "Due Feb 1, 2018\n",
    "\n",
    "One key fact about Artificial Neural Networks is that they are a universal approximator - they can mimic the behavior of any arbitary function.  Let's explore this in the simple case of functions from $R^1 \\to R^1$.\n",
    "\n",
    "Assumptions for the hwk set:\n",
    "- All nodes have their own bias (in other words - not 'one bias for the entire layer)\n",
    "- Input layer uses the identity activation function\n",
    "\n",
    "\n",
    "1. Use the slightly updated starter code below.  Write forward_propagate(), backward_propagate(), descend_gradient().  Use https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ to make certain your code works right.\n",
    "\n",
    "4. Now let's approximate $f(x)=e^{-x^2}$.  Let $X$ be 100 uniformally random numbers between -3 and 3.  Let $Y=f(x)$.  So we have a dataset with $n=100$ obervations, $p=1$ continuous features, and $q=1$ continuous targets.  Use the ANN code we wrote by hand to fit an ANN with 1 hidden layer with 2 nodes using the sigmoid activtion function at each layer except the input.\n",
    " - For consistency, use rnd = np.random.RandomState(seed=42)\n",
    " - Draw your observations first feat = np.sort(rnd.uniform(-3,3,100)).reshape(-1,1).\n",
    " - Pick initial weights rnd.rnd(0,1)\n",
    " - Pick initial bias rnd.rnd(0,1)\n",
    " - At each cycle, record loss / n.  Do enough forward-back-learn cycles to get an ANN that approximates the function well.  Let's set a cutoff of loss / n < 1e-4.\n",
    " - Plot the final ANN approximation on top of the true function.\n",
    " - Plot loss / n vs step\n",
    " \n",
    "7. Repeat this process using different network topologies (#layers and #nodes) and different activation function combinations. Each time, record both # steps and time needed to reach the cutoff.  Try to find the ANN that get a good approximator as efficiently as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "\n",
    "# common activation functions and their derivatives\n",
    "def id(x):\n",
    "    return x\n",
    "\n",
    "def id_deriv(x):\n",
    "    return float(1)\n",
    "\n",
    "def relu(x):\n",
    "    x[x<0] = 0\n",
    "    return x\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x>0).astype(float)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1-tanh(x)**2\n",
    "\n",
    "def arctan(x):\n",
    "    return np.arctan(x)\n",
    "  \n",
    "def arctan_deriv(x):\n",
    "    return 1 / (1 + x**2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    y = np.zeros_like(x)\n",
    "    idx = x > -700\n",
    "    y[idx] = 1 / (1 + np.exp(-1*x[idx]))\n",
    "    return y\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def loss(e):    \n",
    "    return (e**2).sum() / 2\n",
    "\n",
    "def loss_deriv(e):\n",
    "    return e\n",
    "\n",
    "def pre_allocate():\n",
    "    ## pre-allocate\n",
    "    ## X[h,i,j] = input to node j of layer h for observation i; X[h] has shape n x nodes[h]\n",
    "    X = [np.zeros(shape=[n,p]) for p in nodes]\n",
    "\n",
    "    ## Y[h,i,j] = output from node j of layer h for observation i; Y[h] has shape n x nodes[h]\n",
    "    Y = X.copy()\n",
    "\n",
    "    ## B[h,j] = bias into node j of layer h; B[h] has shape nodes[h]\n",
    "    B = [rnd.rand(p) for p in nodes]\n",
    "    B[0] *= 0.0\n",
    "\n",
    "    ## W_sh[h] = (nodes[h], nodes[h+1])\n",
    "    W_sh = [(i,o) for (i,o) in zip(nodes[:-1],nodes[1:])]\n",
    "\n",
    "    ## W[h,j,k] = weight of edge from node j of layer h to node k of layer h+1; W[h] has shape nodes[h] x nodes[h+1]\n",
    "    W = [rnd.rand(*ws) for ws in W_sh]\n",
    "\n",
    "    ## E has shape Y[-1]\n",
    "    E = Y[-1].copy()\n",
    "    \n",
    "    ## DLDX[h,i,j] = partial derivative of loss wrt X[h,i,j]; DLDX[h] has shape n x nodes[h]\n",
    "    DLDX = X.copy()\n",
    "\n",
    "    ## DLDY[h,i,j] = partial derivative of loss wrt Y[h,i,j]; DLDY[h] has shape n x nodes[h]\n",
    "    DLDY = X.copy()\n",
    "\n",
    "    ## DLDB[h,i,j] = partial derivative of loss wrt B[h,j]; DLDB[h] has shape nodes[h]\n",
    "    DLDB = [np.zeros_like(b) for b in B]\n",
    "    \n",
    "    ## DLDW[h,j,k] = partial derivative of loss wrt W[h,j,k]; DLDW[h] has shape nodes[h] x nodes[h+1]\n",
    "    DLDW = [np.zeros_like(w) for w in W]\n",
    "\n",
    "    return X, Y, B, W, E, DLDX, DLDY, DLDB, DLDW\n",
    "\n",
    "## The scaling below is easier with sklearn.preprocessing.  But I wanted to write a purely Numpy algorithm.\n",
    "def minmaxscale(x, a=0, b=1):\n",
    "    m = x.min()\n",
    "    r = x.max() - m\n",
    "    if r == 0:\n",
    "        r = 1\n",
    "    y = (x - m) / r * (b - a) + a\n",
    "    return y\n",
    "\n",
    "\n",
    "def fit_ANN():\n",
    "    global X, Y, B, W, E, DLDX, DLDY, DLDB, DLDW\n",
    "    loss_hist = []\n",
    "    for step in range(max_steps):\n",
    "        X[0] = feat_sc.copy()\n",
    "        forward_propagate()\n",
    "        pred_sc = Y[-1].copy()\n",
    "\n",
    "        E = true_sc - pred_sc  # note: choice of t-p vs p-t may impact the +/- sign in descend_gradient\n",
    "        L = loss(E) / n\n",
    "        loss_hist.append(L)\n",
    "\n",
    "        if loss_hist[-1] < 1e-4:\n",
    "            break\n",
    "\n",
    "        backward_propagate()\n",
    "        descend_gradient()\n",
    "    return loss_hist\n",
    "\n",
    "\n",
    "## functions for pieces of the neural network training\n",
    "def forward_propagate():\n",
    "    global X, Y\n",
    "\n",
    "def backward_propagate():\n",
    "    global DLDX, DLDY, DLDB, DLDW\n",
    "\n",
    "def descend_gradient():\n",
    "    global B, W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## example from https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/, use sigmoid, loss=(e**2)/2 in loss, loss_deriv=e\n",
    "rnd = np.random.RandomState(42)\n",
    "\n",
    "feat = np.array([.05,.1]).reshape(1,-1)\n",
    "true = np.array([.01,.99]).reshape(1,-1)\n",
    "\n",
    "n, p = feat.shape\n",
    "m, q = true.shape\n",
    "if m != n:\n",
    "    raise Exception('feat and true must have the same number of rows')\n",
    "\n",
    "\n",
    "hidden_nodes = [2]\n",
    "max_steps = 1\n",
    "learn_rate = 0.5\n",
    "\n",
    "nodes = [p] + hidden_nodes + [q]\n",
    "layers = len(nodes)\n",
    "\n",
    "## set activation functions\n",
    "activation = [sigmoid for l in range(layers)]\n",
    "activation_deriv = [sigmoid_deriv for l in range(layers)]\n",
    "activation[0] = id\n",
    "activation_deriv[0] = id_deriv\n",
    "# activation[-1] = id\n",
    "# activation_deriv[-1] = id_deriv\n",
    "\n",
    "\n",
    "## Usually should scale data to (0,1) during preprocessing.\n",
    "## Skip for now  to make numbers exactly match Mazur.\n",
    "feat_sc = feat.copy()\n",
    "true_sc = true.copy()\n",
    "\n",
    "X, Y, B, W, E, DLDX, DLDY, DLDB, DLDW = pre_allocate()\n",
    "\n",
    "## example from https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/, use sigmoid, loss=(e**2)/2 in loss, loss_deriv=e\n",
    "W = [np.array([[0.15, 0.25], [0.20, 0.30]]), np.array([[0.40, 0.50], [0.45, 0.55]])]\n",
    "B = [np.array([0.0, 0.0]), np.array([0.35, 0.35]), np.array([0.60, 0.60])]\n",
    "\n",
    "loss_hist = fit_ANN()     \n",
    "# %timeit fit_ANN()\n",
    "\n",
    "for l in range(layers):\n",
    "    print(\"layer = {}\".format(l))\n",
    "    print(\"X\")\n",
    "    print(X[l])\n",
    "    print(\"Y\")\n",
    "    print(Y[l])\n",
    "    print(\"B\")\n",
    "    print(B[l])\n",
    "    print(\"W\")\n",
    "    if l < layers-1:\n",
    "        print(W[l])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
